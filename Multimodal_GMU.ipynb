{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multimodal_GMU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_vJO-RPBu3i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRFR9QgbfOeO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "!pip install --upgrade tables\n",
        "\n",
        "dataset_embeddings = pd.read_hdf('/content/drive/My Drive/dataset/mm_imdb_embeddings.h5', 'embeddings')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q4nIipMCCoh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def Train_Test_Split(data , test_data_fraction = 0.2) :\n",
        "    \n",
        "    mlb = MultiLabelBinarizer()\n",
        "    data_genres_one_hot_encoding = mlb.fit_transform(data['genres'])\n",
        "    Label_names = mlb.classes_\n",
        "    data_genres_one_hot_encoding = pd.DataFrame(data_genres_one_hot_encoding, columns = mlb.classes_)\n",
        "    Data_train, Data_test, Labels_train, Labels_test = train_test_split(data, data_genres_one_hot_encoding, test_size = test_data_fraction)\n",
        "    Labels_train = torch.tensor(Labels_train.values)\n",
        "    Labels_test = torch.tensor(Labels_test.values)\n",
        "    \n",
        "    Data_train = Data_train.reset_index(drop=True)\n",
        "    Data_test = Data_test.reset_index(drop=True)\n",
        "\n",
        "    return (Data_train, Data_test, Labels_train, Labels_test, Label_names)\n",
        "    \n",
        "Data_train, Data_test, Labels_train_tensor, Labels_test_tensor, Label_names = Train_Test_Split(dataset_embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TPjqBNC5hMA6",
        "colab": {}
      },
      "source": [
        "#For multimodal classifier\n",
        "import numpy as np\n",
        "\n",
        "Data_train_tensor_text = torch.tensor(Data_train['bert_embeddings'])\n",
        "Data_test_tensor_text = torch.tensor(Data_test['bert_embeddings'])\n",
        "\n",
        "Data_train_tensor_image = torch.tensor(Data_train['vgg16_embeddings'])\n",
        "Data_test_tensor_image = torch.tensor(Data_test['vgg16_embeddings'])\n",
        "\n",
        "#torch.tensor(np.vstack(Data_train['vgg16_embeddings'].values[:]).astype(np.float))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvTtOuVlafMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "#source: https://github.com/Duncanswilson/maxout-pytorch/blob/master/maxout_pytorch.ipynb\n",
        "class ListModule(object):\n",
        "    def __init__(self, module, prefix, *args):\n",
        "        self.module = module\n",
        "        self.prefix = prefix\n",
        "        self.num_module = 0\n",
        "        for new_module in args:\n",
        "            self.append(new_module)\n",
        "\n",
        "    def append(self, new_module):\n",
        "        if not isinstance(new_module, nn.Module):\n",
        "            raise ValueError('Not a Module')\n",
        "        else:\n",
        "            self.module.add_module(self.prefix + str(self.num_module), new_module)\n",
        "            self.num_module += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_module\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        if i < 0 or i >= self.num_module:\n",
        "            raise IndexError('Out of bound')\n",
        "        return getattr(self.module, self.prefix + str(i))\n",
        "\n",
        "\n",
        "class Maxout_MLP(nn.Module):\n",
        "    def __init__(self, hidden_layer_size1, hidden_layer_size2, dropout, num_maxout_units=2):\n",
        "        super(Maxout_MLP, self).__init__()\n",
        "        self.fc1_list = ListModule(self, \"fc1_\")\n",
        "        self.fc2_list = ListModule(self, \"fc2_\")\n",
        "        self.hidden_layer_size1 = hidden_layer_size1\n",
        "        self.hidden_layer_size2 = hidden_layer_size2\n",
        "        for _ in range(num_maxout_units):\n",
        "            self.fc1_list.append(nn.Linear(self.hidden_layer_size1, self.hidden_layer_size2))\n",
        "            self.fc2_list.append(nn.Linear(self.hidden_layer_size2, self.hidden_layer_size2))\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x): \n",
        "        x = x.view(-1, self.hidden_layer_size1)\n",
        "        x = self.maxout(x, self.fc1_list)\n",
        "        x = self.dropout(x)\n",
        "        #x = self.maxout(x, self.fc2_list)\n",
        "        return x\n",
        "\n",
        "    def maxout(self, x, layer_list):\n",
        "        max_output = layer_list[0](x)\n",
        "        for _, layer in enumerate(layer_list, start=1):\n",
        "            max_output = torch.max(max_output, layer(x))\n",
        "        return max_output\n",
        "\n",
        "\n",
        "class GMU(nn.Module):\n",
        "\n",
        "    def __init__(self, num_maxout_units = 2, hidden_layer_size = 512, text_embeddings_size = 768, img_embeddings_size = 4096, num_labels = 23, hidden_activation = None, dropout = 0.1):\n",
        "\n",
        "        super(GMU, self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.hidden_layer_size = hidden_layer_size\n",
        "\n",
        "        self.linear_h_text = torch.nn.Linear(text_embeddings_size, self.hidden_layer_size, bias = False)\n",
        "        self.linear_h_image = torch.nn.Linear(img_embeddings_size, self.hidden_layer_size, bias = False)\n",
        "        self.linear_z = torch.nn.Linear(text_embeddings_size + img_embeddings_size, self.hidden_layer_size, bias = False)\n",
        "\n",
        "        self.tanh = torch.nn.Tanh()\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.linear = torch.nn.Linear(self.hidden_layer_size, self.num_labels)\n",
        "        \n",
        "        self.maxout = Maxout_MLP(self.hidden_layer_size, self.hidden_layer_size, dropout, num_maxout_units=num_maxout_units)\n",
        "\n",
        "    def forward(self, image_embeddings, text_embeddings):\n",
        "        \n",
        "        image_h = self.linear_h_image(image_embeddings)\n",
        "        image_h = self.tanh(image_h)\n",
        "        text_h = self.linear_h_text(text_embeddings)\n",
        "        text_h = self.tanh(text_h)\n",
        "        concat = torch.cat((image_embeddings, text_embeddings), 1)\n",
        "        z = self.linear_z(concat)\n",
        "        z = self.sigmoid(z)\n",
        "        gmu_output = z*image_h + (1-z)*text_h\n",
        "        \n",
        "        maxout_mlp_output = self.maxout(gmu_output)\n",
        "        #dropped_layer = self.dropout(gmu_output)\n",
        "\n",
        "        logits = self.linear(maxout_mlp_output)\n",
        "        if(self.training) :\n",
        "            return logits\n",
        "        else :\n",
        "            output = self.sigmoid(logits)\n",
        "            return output\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZpSHxxYhVF5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from tqdm import tqdm, trange\n",
        "!pip install transformers\n",
        "from transformers import BertConfig, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "\n",
        "class Training_Testing_MM():\n",
        "\n",
        "    def __init__(self, Data_train_tensor_text, Data_train_tensor_image, Labels_train_tensor, \n",
        "                 Data_test_tensor_text, Data_test_tensor_image, Labels_test_tensor, \n",
        "                 Label_names = None, hidden_layer_size = 4864, num_maxout_units = 2,\n",
        "                 hidden_activation = \"tanh\", batch_size = 32, epochs = 10, sigmoid_thresh = 0.2, learning_rate = 2e-5, num_labels = 23, dropout = 0.1):\n",
        "\n",
        "\n",
        "      #self.dropout = dropout\n",
        "      #self.hidden_layer_size = hidden_layer_size\n",
        "      #self.hidden_activation = hidden_activation\n",
        "      self.model = GMU(num_maxout_units = num_maxout_units, hidden_layer_size = hidden_layer_size, hidden_activation = hidden_activation, dropout = dropout).cuda()\n",
        "      self.label_names = Label_names\n",
        "      self.num_labels = num_labels\n",
        "      self.batch_size = batch_size\n",
        "      self.learning_rate = learning_rate\n",
        "      self.epochs = epochs\n",
        "      self.sigmoid_thresh = sigmoid_thresh\n",
        "      self.optimizer = self.SetOptimizer()\n",
        "      self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "      self.results = pd.DataFrame(0, index=['Recall','Precision','F_Score'], columns=['micro', 'macro', 'weighted', 'samples']).astype(float)\n",
        "      self.epoch_loss_set = []\n",
        "      self.train_dataloader = self.SetTrainDataloader_MM(Data_train_tensor_text, Data_train_tensor_image, Labels_train_tensor)\n",
        "      self.test_dataloader = self.SetTestDataloader_MM(Data_test_tensor_text, Data_test_tensor_image, Labels_test_tensor) \n",
        "      self.scheduler = self.SetScheduler()\n",
        "      \n",
        "\n",
        "    def SetOptimizer(self) :\n",
        "\n",
        "      optimizer = AdamW(self.model.parameters(), self.learning_rate, eps = 1e-6)\n",
        "      return(optimizer)\n",
        "\n",
        "    \n",
        "\n",
        "    def SetScheduler(self) :\n",
        "\n",
        "      scheduler = get_linear_schedule_with_warmup(self.optimizer, num_warmup_steps = 0, \n",
        "                                                  num_training_steps = self.epochs*len(self.train_dataloader))\n",
        "      return(scheduler) \n",
        "\n",
        "\n",
        "\n",
        "    def Get_Metrics(self, actual, predicted) :\n",
        "\n",
        "      #acc = metrics.accuracy_score(actual, predicted)\n",
        "      #hamming = metrics.hamming_loss(actual, predicted)\n",
        "      #(metrics.roc_auc_score(actual, predicted, average=average)\n",
        "      averages = ('micro', 'macro', 'weighted', 'samples')\n",
        "      for average in averages:\n",
        "          precision, recall, fscore, _ = metrics.precision_recall_fscore_support(actual, predicted, average=average)\n",
        "          self.results[average]['Recall'] += recall\n",
        "          self.results[average]['Precision'] += precision\n",
        "          self.results[average]['F_Score'] += fscore\n",
        "\n",
        "\n",
        "\n",
        "    #source: https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "    def Plot_Training_Epoch_Loss(self) :\n",
        "\n",
        "      sns.set(style='darkgrid')\n",
        "      sns.set(font_scale=1.5)\n",
        "      plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "      plt.plot(self.epoch_loss_set, 'b-o')\n",
        "      plt.title(\"Training loss\")\n",
        "      plt.xlabel(\"Epoch\")\n",
        "      plt.ylabel(\"Loss\")\n",
        "      plt.savefig('Training_Epoch_Loss.png',bbox_inches='tight')\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "    #source: https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "    def format_time(self, elapsed):\n",
        "      '''\n",
        "      Takes a time in seconds and returns a string hh:mm:ss\n",
        "      '''\n",
        "      # Round to the nearest second.\n",
        "      elapsed_rounded = int(round((elapsed)))\n",
        "      return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "\n",
        "    def SetTrainDataloader_MM(self, Data_train_tensor_text, Data_train_tensor_image, Labels_train_tensor) :\n",
        "\n",
        "      train_dataset = TensorDataset(Data_train_tensor_text, Data_train_tensor_image, Labels_train_tensor)\n",
        "      train_sampler = RandomSampler(train_dataset)\n",
        "      train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size = self.batch_size)\n",
        "      return(train_dataloader)\n",
        "\n",
        "\n",
        "    def SetTestDataloader_MM(self, Data_test_tensor_text, Data_test_tensor_image, Labels_test_tensor) :\n",
        "      \n",
        "      test_dataset = TensorDataset(Data_test_tensor_text, Data_test_tensor_image, Labels_test_tensor)\n",
        "      test_sampler = SequentialSampler(test_dataset)\n",
        "      #test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size = self.batch_size)\n",
        "      test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size = Data_test_tensor_text.shape[0])\n",
        "      return(test_dataloader)\n",
        "\n",
        "   \n",
        "    def Train(self) :\n",
        "\n",
        "      for _ in trange(self.epochs, desc=\"Epoch\"):\n",
        "        \n",
        "        self.model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        # Measure how long the training epoch takes.\n",
        "        t0 = time.time()\n",
        "    \n",
        "        for step_num, batch_data in enumerate(self.train_dataloader):\n",
        "\n",
        "          # Progress update every 30 batches.\n",
        "          if step_num % 30 == 0 and not step_num == 0:\n",
        "            elapsed = self.format_time(time.time() - t0)\n",
        "            print('  Batch : ',step_num, ' , Time elapsed : ',elapsed)\n",
        "\n",
        "          samples_text, samples_image, labels = tuple(t.to(self.device) for t in batch_data)\n",
        "          self.optimizer.zero_grad()\n",
        "          logits = self.model(samples_image.float(), samples_text.float())\n",
        "          loss_fct = BCEWithLogitsLoss()\n",
        "          batch_loss = loss_fct(logits.view(-1, self.num_labels).float(), labels.view(-1, self.num_labels).float())\n",
        "          batch_loss.backward()\n",
        "          self.optimizer.step()\n",
        "          self.scheduler.step()\n",
        "          epoch_loss += batch_loss.item()\n",
        "\n",
        "        avg_epoch_loss = epoch_loss/len(self.train_dataloader)\n",
        "        print(\"\\nTrain loss for epoch: \",avg_epoch_loss)\n",
        "        print(\"\\nTraining epoch took: {:}\".format(self.format_time(time.time() - t0)))\n",
        "        self.epoch_loss_set.append(avg_epoch_loss)\n",
        "\n",
        "      torch.save(self.model.state_dict(), \"/content/drive/My Drive/dataset/model.pt\")\n",
        "      self.Plot_Training_Epoch_Loss()\n",
        "\n",
        "\n",
        "\n",
        "    def Test(self) :\n",
        "\n",
        "      # Put model in evaluation mode to evaluate loss on the test set\n",
        "      self.model.eval()\n",
        "\n",
        "      for batch_data in self.test_dataloader:\n",
        "  \n",
        "        samples_text, samples_image, labels = tuple(t.to(self.device) for t in batch_data)\n",
        "      \n",
        "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "        # Forward pass, calculate logit predictions\n",
        "        with torch.no_grad():\n",
        "          output = self.model(samples_image.float(), samples_text.float())\n",
        "\n",
        "        threshold = torch.Tensor([self.sigmoid_thresh]).to(self.device)\n",
        "        predictions = (output > threshold).int()\n",
        "\n",
        "        # Move preds and labels to CPU\n",
        "        predictions = predictions.detach().cpu().numpy()\n",
        "        labels = labels.to('cpu').numpy()\n",
        "      \n",
        "        self.Get_Metrics(labels, predictions)\n",
        "    \n",
        "      self.results = self.results/len(self.test_dataloader)\n",
        "      #print(\"Test data metrics : \\n\")\n",
        "\n",
        "      print(\"\\nGenres with no predicted samples : \", self.label_names[np.where(np.sum(predictions, axis=0) == 0)[0]])\n",
        "      \n",
        "      return(self.results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xARlxJVfiPhH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_test = Training_Testing_MM(Data_train_tensor_text, Data_train_tensor_image, Labels_train_tensor, Data_test_tensor_text, Data_test_tensor_image, Labels_test_tensor, Label_names=Label_names,\n",
        "                                 hidden_layer_size = 2048, epochs = 50, batch_size= 512, learning_rate=0.0005, dropout=0.1, sigmoid_thresh = 0.2, num_maxout_units = 20)\n",
        "train_test.Train()\n",
        "train_test.Test()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}