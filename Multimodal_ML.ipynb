{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multimodal_ML.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JasonArmitage-res/Lab_Multimodal_ML/blob/master/Multimodal_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKqONbvNKYjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "'''\n",
        "USAGE:\n",
        "\n",
        "data = Read(folder_path)\n",
        "ViewDataDistribution(data)\n",
        "data.to_pickle(\"./mm_imdb.pkl\")\n",
        "\n",
        "'''\n",
        "def Read(path) :\n",
        "    im_files = os.listdir(path)\n",
        "    current_directory = os.getcwd()\n",
        "    os.chdir(path)\n",
        "   \n",
        "    #images_cv will return list of numpy arrays containing RGB values\n",
        "    images_cv = []\n",
        "    for a in im_files: # Iterate over a copy of the list\n",
        "        if a.endswith(\".jpeg\"):\n",
        "            images_cv.append(Image.open(a).resize((256,256)).convert('RGB'))\n",
        "\n",
        "    images_cv = pd.DataFrame(images_cv, columns = ['image'])\n",
        "    \n",
        "    json_files = [pos_json for pos_json in os.listdir(path) if pos_json.endswith('.json')]\n",
        "    jsons_data = pd.DataFrame(columns=['genres', 'plot', 'fileID'])\n",
        "    for index, js in enumerate(json_files):\n",
        "        with open(os.path.join(path, js)) as json_file:\n",
        "            json_text = json.load(json_file)\n",
        "            genres = json_text['genres']\n",
        "            plot = json_text['plot'][0]\n",
        "            fileID = js.rstrip(\".json\")\n",
        "            #jsons_data will return a dataframe with genres,file names and plot\n",
        "            jsons_data.loc[index] = [genres,plot,fileID]\n",
        "           \n",
        "    os.chdir(current_directory)\n",
        "   \n",
        "    return (pd.concat([jsons_data, images_cv], axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGdUqT2SK3c4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ViewDataDistribution(data):\n",
        "    \n",
        "    all_genres = sum(data[\"genres\"],[])\n",
        "    unique_genres = (set(all_genres))\n",
        "    all_genres = nltk.FreqDist(all_genres)\n",
        "    all_genres_df = pd.DataFrame({'Genre': list(all_genres.keys()), \n",
        "                              'Count': list(all_genres.values())})\n",
        "    plt.figure(figsize=(15,12)) \n",
        "    ax = sns.barplot(data=all_genres_df, x= \"Count\", y = \"Genre\") \n",
        "    for p in ax.patches:\n",
        "        ax.annotate(\"%d\" % p.get_width(), (p.get_x() + p.get_width(), p.get_y() + 0.5), xytext=(5,0), textcoords='offset points')\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def RemoveGenresFromData(data):\n",
        "    \n",
        "    genres_to_remove = [\"Adult\",\"News\",\"Talk-Show\",\"Reality-TV\"]\n",
        "    data_genres_removed = data[~np.array([bool(set(genre) & set(genres_to_remove)) for genre in data[\"genres\"]])] \n",
        "    \n",
        "    return(data_genres_removed)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def MoviesPerGenre(data):\n",
        "    \n",
        "    data_single_genre = pd.DataFrame({\n",
        "                                  col:np.repeat(data[col].values, data[\"genres\"].str.len())\n",
        "                                  for col in data.columns.drop(\"genres\")}\n",
        "                                ).assign(**{\"genres\":np.concatenate(data[\"genres\"].values)})[data.columns]\n",
        "    fileID_by_genre = data_single_genre.groupby(\"genres\")[\"fileID\"].apply(list).reset_index(name='fileIDs')\n",
        "    \n",
        "    return(fileID_by_genre)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KITc_A_ELCO4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "USAGE:\n",
        "\n",
        "samples = SamplingByCount(data,338)\n",
        "ViewDataDistribution(samples)\n",
        "samples.to_pickle(\"./mm_imdb_sampled.pkl\")\n",
        "\n",
        "'''\n",
        "def SamplingByCount (data, count = 330) :\n",
        "    \n",
        "    data_subset = RemoveGenresFromData(data)\n",
        "    fileID_by_genre = MoviesPerGenre(data_subset)\n",
        "    samples_fileIDs = []\n",
        "\n",
        "    for index, row in fileID_by_genre.iterrows():\n",
        "    \n",
        "        samples_fileIDs.extend(random.sample(row[\"fileIDs\"],count))\n",
        "        \n",
        "    data_sampled = data_subset[data_subset[\"fileID\"].isin(samples_fileIDs)]\n",
        "    \n",
        "    return (data_sampled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyQmJrxjLYHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "'''\n",
        "USAGE:\n",
        "\n",
        "Data_train, Data_test, Labels_train_tensor, Labels_test_tensor = Train_Test_Split(data)\n",
        "\n",
        "'''\n",
        "\n",
        "def Train_Test_Split(data , test_data_fraction = 0.2) :\n",
        "    \n",
        "    mlb = MultiLabelBinarizer()\n",
        "    data_genres_one_hot_encoding = mlb.fit_transform(data['genres'])\n",
        "    Label_names = mlb.classes_\n",
        "    data_genres_one_hot_encoding = pd.DataFrame(data_genres_one_hot_encoding, columns = mlb.classes_)\n",
        "    Data_train, Data_test, Labels_train, Labels_test = train_test_split(data, data_genres_one_hot_encoding, test_size = test_data_fraction)\n",
        "    Labels_train = torch.tensor(Labels_train.values)\n",
        "    Labels_test = torch.tensor(Labels_test.values)\n",
        "\n",
        "    return (Data_train, Data_test, Labels_train, Labels_test, Label_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4KGtqO7LZIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from transformers import BertTokenizer\n",
        "from nltk import tokenize\n",
        "#nltk.download('punkt')\n",
        "\n",
        "\n",
        "'''\n",
        "USAGE:\n",
        "\n",
        "bert_input = BertInput()\n",
        "input_ids = list(Data_train['plot'].apply(lambda x: bert_input.GenerateBertInput(x)))\n",
        "input_ids = pd.DataFrame(input_ids,columns=['indexed_tokens','segment_ids','masked_ids'])\n",
        "Data_train_tensor = torch.tensor(input_ids['indexed_tokens'])\n",
        "torch.save(Data_train_tensor, '/content/drive/My Drive/Dataset/Data_train_tensor.pt')\n",
        "\n",
        "\n",
        "input_ids = list(Data_test['plot'].apply(lambda x: bert_input.GenerateBertInput(x)))\n",
        "input_ids = pd.DataFrame(input_ids,columns=['indexed_tokens','segment_ids','masked_ids'])\n",
        "Data_test_tensor = torch.tensor(input_ids['indexed_tokens'])\n",
        "torch.save(Data_test_tensor, '/content/drive/My Drive/Dataset/Data_test_tensor.pt')\n",
        "\n",
        "'''\n",
        "\n",
        "#Generates formatted input (for Bert) from text\n",
        "class BertInput () :\n",
        "    \n",
        "    def __init__(self, max_input_length = 512):\n",
        "\n",
        "        self.indexed_tokens = []\n",
        "        self.segment_ids = []\n",
        "        self.masked_ids = []\n",
        "        self.max_input_length = max_input_length\n",
        "\n",
        "    def GetIndexedTokens(self, text):\n",
        "        \n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        #tagged_text = self.AddSpecialTokens(text)\n",
        "        #tokenized_text = tokenizer.tokenize(tagged_text)\n",
        "        tokenized_text = tokenizer.tokenize(text)\n",
        "        tokenized_text.append(\"[SEP]\")\n",
        "        tokenized_text.insert(0,\"[CLS]\")\n",
        "        self.indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "    '''\n",
        "    def AddSpecialTokens (self, text) : \n",
        "    \n",
        "        #add [CLS] token in the beggining of every text, [SEP] after every sentence\n",
        "        sentences = tokenize.sent_tokenize('[CLS] ' + text)\n",
        "        sentences = [x + ' [SEP]' for x in sentences]\n",
        "        return (' '.join(sentences))\n",
        "    '''\n",
        "\n",
        "    def GetSegmentIds(self) :\n",
        "        \n",
        "        self.segment_ids = [1] * len(self.indexed_tokens)\n",
        " \n",
        "    def GetMaskedIds(self) :\n",
        "        \n",
        "        self.masked_ids = [1] * len(self.indexed_tokens)\n",
        "\n",
        "    def Padding(self) :\n",
        "\n",
        "        if(len(self.indexed_tokens) < self.max_input_length) :\n",
        "           padding = [0]*(self.max_input_length - len(self.indexed_tokens))\n",
        "           self.indexed_tokens += padding\n",
        "           self.segment_ids += padding\n",
        "           self.masked_ids += padding\n",
        "        else :\n",
        "           del self.indexed_tokens[self.max_input_length:]\n",
        "           del self.segment_ids[self.max_input_length:]\n",
        "           del self.masked_ids[self.max_input_length:]\n",
        "\n",
        "    def GenerateBertInput(self, text) :\n",
        "\n",
        "        self.GetIndexedTokens(text)\n",
        "        self.GetSegmentIds()\n",
        "        self.GetMaskedIds()\n",
        "        self.Padding()\n",
        "        #result = [torch.tensor(self.indexed_tokens),torch.tensor(self.segment_ids),torch.tensor(self.masked_ids)]\n",
        "        \n",
        "        return [self.indexed_tokens, self.segment_ids, self.masked_ids]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7IW0omJLiNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ref: https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d\n",
        "from transformers import BertModel\n",
        "from torch import nn\n",
        "\n",
        "class BertMultiLabelClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, config, num_labels = 23, dropout = 0.1):\n",
        "\n",
        "        super(BertMultiLabelClassifier, self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.base_model = BertModel.from_pretrained('bert-base-uncased', config=config)\n",
        "        self.hidden_layer = torch.nn.Linear(config.hidden_size, num_labels)\n",
        "        self.output_layer = torch.nn.Sigmoid()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, indexed_tokens, segment_ids=None, masked_ids=None):\n",
        "        \n",
        "        pooled_output = self.base_model(indexed_tokens, segment_ids, masked_ids)\n",
        "        dropped_layer = self.dropout(pooled_output[1])\n",
        "        logits = self.hidden_layer(dropped_layer)\n",
        "        if(self.training) :\n",
        "            return logits\n",
        "        else :\n",
        "            output = self.output_layer(logits)\n",
        "            return output\n",
        "\n",
        "\n",
        "    def freeze_bert_encoder(self):\n",
        "        for param in self.base_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "    def unfreeze_bert_encoder(self):\n",
        "        for param in self.base_model.named_parameters():\n",
        "            param.requires_grad = True\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mupMJvBDLm-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from tqdm import tqdm, trange\n",
        "#!pip install transformers\n",
        "from transformers import BertConfig, AdamW\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "\n",
        "class Training_Testing_Bert():\n",
        "\n",
        "    def __init__(self, Data_train_tensor, Labels_train_tensor, Data_test_tensor, Labels_test_tensor, \n",
        "                 Label_names = None, batch_size = 16, epochs = 5, freeze_bert = True, sigmoid_thresh = 0.2, \n",
        "                 optim_lr = 2e-5, num_labels = 23):\n",
        "      \n",
        "      self.config = BertConfig.from_pretrained('bert-base-uncased')\n",
        "      self.bert = BertMultiLabelClassifier(config).cuda()\n",
        "      \n",
        "      #do not train the Bert model\n",
        "      if(freeze_bert) :\n",
        "        self.bert.freeze_bert_encoder()\n",
        "\n",
        "      self.label_names = Label_names\n",
        "      self.num_labels = num_labels\n",
        "      self.batch_size = batch_size\n",
        "      self.optim_lr = optim_lr\n",
        "      self.epochs = epochs\n",
        "      self.sigmoid_thresh = sigmoid_thresh\n",
        "      self.optimizer = self.SetOptimizer()\n",
        "      #self.scheduler = self.SetScheduler()\n",
        "      self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "      self.results = pd.DataFrame(0, index=['Recall','Precision','F_Score'], columns=['micro', 'macro', 'weighted', 'samples']).astype(float)\n",
        "      self.epoch_loss_set = []\n",
        "      self.train_dataloader = self.SetTrainDataloader(Data_train_tensor, Labels_train_tensor)\n",
        "      self.test_dataloader = self.SetTestDataloader(Data_test_tensor, Labels_test_tensor)\n",
        "\n",
        "\n",
        "    def SetOptimizer(self) :\n",
        "\n",
        "      optimizer = AdamW(self.bert.parameters(), self.optim_lr, eps = 1e-6)\n",
        "      return(optimizer)\n",
        "\n",
        "    \n",
        "    '''\n",
        "    def SetScheduler(self) :\n",
        "\n",
        "      scheduler =\n",
        "      return(scheduler) \n",
        "    '''\n",
        "\n",
        "\n",
        "    def Get_Metrics(self, actual, predicted) :\n",
        "\n",
        "      #acc = metrics.accuracy_score(actual, predicted)\n",
        "      #hamming = metrics.hamming_loss(actual, predicted)\n",
        "      #(metrics.roc_auc_score(actual, predicted, average=average)\n",
        "      averages = ('micro', 'macro', 'weighted', 'samples')\n",
        "      for average in averages:\n",
        "          precision, recall, fscore, _ = metrics.precision_recall_fscore_support(actual, predicted, average=average)\n",
        "          self.results[average]['Recall'] += recall\n",
        "          self.results[average]['Precision'] += precision\n",
        "          self.results[average]['F_Score'] += fscore\n",
        "\n",
        "\n",
        "    #source: https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "    def Plot_Training_Epoch_Loss(self) :\n",
        "\n",
        "      sns.set(style='darkgrid')\n",
        "      sns.set(font_scale=1.5)\n",
        "      plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "      plt.plot(self.epoch_loss_set, 'b-o')\n",
        "      plt.title(\"Training loss\")\n",
        "      plt.xlabel(\"Epoch\")\n",
        "      plt.ylabel(\"Loss\")\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "    #source: https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "    def format_time(self, elapsed):\n",
        "      '''\n",
        "      Takes a time in seconds and returns a string hh:mm:ss\n",
        "      '''\n",
        "      # Round to the nearest second.\n",
        "      elapsed_rounded = int(round((elapsed)))\n",
        "      return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "\n",
        "    def SetTrainDataloader(self, Data_train_tensor, Labels_train_tensor) :\n",
        "\n",
        "      train_dataset = TensorDataset(Data_train_tensor, Labels_train_tensor)\n",
        "      train_sampler = RandomSampler(train_dataset)\n",
        "      train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size = self.batch_size)\n",
        "      return(train_dataloader)\n",
        "\n",
        "\n",
        "    def SetTestDataloader(self, Data_test_tensor, Labels_test_tensor) :\n",
        "      \n",
        "      test_dataset = TensorDataset(Data_test_tensor, Labels_test_tensor)\n",
        "      test_sampler = SequentialSampler(test_dataset)\n",
        "      test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size = self.batch_size)\n",
        "      return(test_dataloader)\n",
        "\n",
        "\n",
        "\n",
        "    def Train_Bert(self) :\n",
        "\n",
        "      for _ in trange(self.epochs, desc=\"Epoch\"):\n",
        "        \n",
        "        self.bert.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        # Measure how long the training epoch takes.\n",
        "        t0 = time.time()\n",
        "    \n",
        "        for step_num, batch_data in enumerate(self.train_dataloader):\n",
        "\n",
        "          # Progress update every 30 batches.\n",
        "          if step_num % 30 == 0 and not step_num == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch : ',step_num, ' , Time elapsed : ',elapsed)\n",
        "\n",
        "          token_ids, labels = tuple(t.to(self.device) for t in batch_data)\n",
        "          self.optimizer.zero_grad()\n",
        "          logits = self.bert(token_ids)\n",
        "          loss_fct = BCEWithLogitsLoss()\n",
        "          batch_loss = loss_fct(logits.view(-1, self.num_labels).float(), labels.view(-1, self.num_labels).float())\n",
        "          batch_loss.backward()\n",
        "          #scheduler.step()\n",
        "          self.optimizer.step()\n",
        "          epoch_loss += batch_loss.item()\n",
        "\n",
        "      avg_epoch_loss = epoch_loss/len(self.train_dataloader)\n",
        "      print(\"\\nTrain loss for epoch: \",avg_epoch_loss)\n",
        "      print(\"\\nTraining epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "      self.epoch_loss_set.append(avg_epoch_loss)\n",
        "\n",
        "      torch.save(self.bert.state_dict(), \"/content/drive/My Drive/Dataset/bert_unimodal.pt\")\n",
        "      self.Plot_Training_Epoch_Loss()\n",
        "    \n",
        "\n",
        "    def Test_Bert(self) :\n",
        "\n",
        "      # Put model in evaluation mode to evaluate loss on the test set\n",
        "      self.bert.eval()\n",
        "\n",
        "      for batch_data in self.test_dataloader:\n",
        "  \n",
        "        token_ids, labels = tuple(t.to(device) for t in batch_data)\n",
        "      \n",
        "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "        # Forward pass, calculate logit predictions\n",
        "        with torch.no_grad():\n",
        "          output = self.bert(token_ids)\n",
        "\n",
        "        threshold = torch.Tensor([self.sigmoid_thresh]).to(self.device)\n",
        "        predictions = (output > threshold).int()\n",
        "\n",
        "        # Move preds and labels to CPU\n",
        "        predictions = predictions.detach().cpu().numpy()\n",
        "        labels = labels.to('cpu').numpy()\n",
        "      \n",
        "        self.Get_Metrics(labels, predictions)\n",
        "    \n",
        "      self.results = self.results/len(self.test_dataloader)\n",
        "      print(\"Test data metrics : \\n\")\n",
        "      self.results\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}